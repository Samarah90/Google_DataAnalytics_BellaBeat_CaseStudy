# CASE STUDY: Bellabeat Fitness Data Analysis 
##### Author: Samarah Rizvi

##### Date: June 5, 2023

This case study follows the follwing six data analysis steps:
### 1- Ask
### 2- Prepare
### 3- Process
### 4- Analyze
### 5- Share
### 6- Act

# Background of bellabeat
Bellabeat is a high-tech company that manufactures health-focused smart products. Since it was founded in 2013, Bellabeat has grown rapidly and quickly positioned itself as a tech-driven wellness company for women. The company has 5 main products: bellabeat app, leaf(basic wellness tracker), time(wellness watch), spring(smart water bottle) and bellabeat membership. Bellabeat is a successful small company, but they have the potential to become a larger player in the global smart device market. 
## 1- Ask phase
### Business Task : We are asked to analyze Fitbit smart device data to gain insight into how consumers are using their smart devices. The insights can help guide the marketing strategy for the company. 
Main stakeholders: Urška Sršen(cofounder and Chief Creative Officer), Sando Mur (Mathematician and Bellabeat’s cofounder) and Bellabeat marketing analytics team.
## 2- Prepare phase 
### Dataset source
The data source used for our case study is the FitBit Fitness Tracker Data. This is a 30 participants FitBit Fitness Tracker Data avaiable from Mobius: https://www.kaggle.com/arashnic/fitbit. It is an open source data. the dataset has 18 csv files.
### Does the data follow the ROCCC approach? 
- Reliability and originality:
The data is from 30 FitBit users who consented to the submission of personal tracker data and generated by from a distributed survey via Amazon Mechanical Turk.
- Comprehensive:  Thirty eligible Fitbit users consented to the submission of personal tracker data, including minute-level output for physical activity, heart rate, and sleep monitoring.
- Current: These datasets were generated by respondents to a distributed survey via Amazon Mechanical Turk between 03.12.2016-05.12.2016.
- Cited : unknown
### Data Credibility and Integrity
The sample size of the data is very small and the absence of the demographic information of the participents can make the data prone to the sampling bias which means we are unsure if the sample size is representative of the whole population as a whole. Another problem is that the dataset is not current (recorded in 2016). 
## 3- Process phase
I have focused on the following datasets for my analysis:
- dailyActivity_merged
- hourlySteps_merged
- sleepDay_merged
- weightLogInfo_merged
### installing and loading all the required libraries
```
install.packages("readr")
install.packages("here")
install.packages("skimr")
install.packages("janitor")
install.packages("dplyr")
install.packages("tidyverse")
install.packages("lubridate")
install.packages("ggplot2")
install.packages("ggpubr")
```
```
library(readr)
library(here)
library(skimr)
library(janitor)
library(dplyr)
library(tidyverse)
library(lubridate)
library(ggpubr)
library(ggplot2)
```
### 1- The dailyActivity_merged dataset
#### summary
```
skim_without_charts(dailyActivity_merged)
glimpse(dailyActivity_merged)
```
#### Change date format
```
dailyActivity_merged$ActivityDate <- mdy(dailyActivity_merged$ActivityDate)
```
#### Count the missing, duplicated and distinct Ids
```
dim(dailyActivity_merged)
sum(is.na(dailyActivity_merged)) 
sum(duplicated(dailyActivity_merged)) 
n_distinct(dailyActivity_merged$Id)
```
No missing or duplicated values. 33 distinct Ids instead of the reported 30 distinct Ids.
#### Making a new dataset of daily_activity with new columns of Total active hours, Total inactive hours and Weekdays
```
daily_activity <- dailyActivity_merged %>% mutate(Total_active_hours = c(VeryActiveMinutes + FairlyActiveMinutes + LightlyActiveMinutes)/60) %>%
    mutate( Weekday = weekdays(as.Date(ActivityDate, "%m/%d/%Y"))) %>% mutate(Total_sedentary_hours = SedentaryMinutes/60)
daily_activity <- daily_activity %>% mutate(Total_active_hours = round(Total_active_hours, digits = 0)) #round the hours#
daily_activity <- daily_activity %>% mutate(Total_sedentary_hours = round(Total_sedentary_hours, digits = 0))
daily_activity <- daily_activity[,c (1,2,17,3,4,16,18,15,5,6,7,8,9,10,11,12,13,14)] 
colnames(daily_activity)
```
### 2- The hourleySteps_merged dataset
#### summary
```
glimpse(hourlySteps_merged)
sum(duplicated(hourlySteps_merged)) #0 duplicated#
n_distinct(hourlySteps_merged$Id) #33 ids#
```
No missing or duplicated values. 33 distinct Ids instead of the reported 30 distinct Ids.
#### Making a new dataset of HourleySteps with seperate date time column using the lubridate package's parse_date_time()
```
Hourlysteps <- hourlySteps_merged %>% #sep date time column#
  mutate(ActivityHour = parse_date_time(ActivityHour, "m/d/y I:M:S p")) %>% 
  mutate(ActivityDate = as.Date(ActivityHour), ActivityHour = format(ActivityHour, format = "%H:%M:%S"))
```
### 3- The SleepDay_merged datset
#### summary
```
skim_without_charts(sleepDay_merged)
glimpse(sleepDay_merged)
```
#### Count the missing, duplicated and distinct Ids
```
dim(sleepDay_merged)
sum(is.na(sleepDay_merged)) #no missing value
sum(duplicated(sleepDay_merged)) #3 duplicates#
sleep_day <- sleepDay_merged[!duplicated(sleepDay_merged), ]
sum(duplicated(sleep_day)) #no duplicated values#
n_distinct(sleepDay_merged$Id) #24 distant ids#
```
No missing values but three duplicated values. So made a new dataset of Sleep_day with the removed duplicated values. 24 distinct Ids instead of 33 as in the previous datasets.
#### seperate date time column using the lubridate package's parse_date_time(), new column of weekdays from the date and new column for sleep in hours
```
sleep_day <- sleepDay_merged %>% mutate(SleepDay = parse_date_time(SleepDay, "m/d/y I:M:S p")) %>%  
  mutate(SleepDate = as.Date(SleepDay), SleepTime = format(SleepDay, format = "%H:%M:%S")) %>% 
  mutate( SleepWeekday = weekdays(as.Date(SleepDate, "%m/%d/%Y"))) %>% 
  mutate(TotalHoursAsleep = TotalMinutesAsleep/60) %>% 
  mutate(TotalHoursInBed = TotalTimeInBed/60) 
sleep_day <- sleep_day %>% select(-SleepDay)
```
### 4- The weightLogInfo_merged datset
#### summary
```
skim_without_charts(weightLogInfo_merged)
glimpse(weightLogInfo_merged)
```
#### Count the missing, duplicated and distinct Ids
```
sum(is.na(weightLogInfo_merged)) 
sum(duplicated(weightLogInfo_merged)) 
n_distinct(weightLogInfo_merged$Id) 
```
There are 65 missing values from the column Fat and only 8 distinct Ids with no duplicates. I have dropped this dataset from my further analysis as the sample size is very small.
